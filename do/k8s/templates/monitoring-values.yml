## Stack name: prometheus-community/kube-prometheus-stack
## Ref: https://github.com/prometheus-community/helm-charts/tree/kube-prometheus-stack-35.5.1/charts/kube-prometheus-stack
##


## Manages Prometheus and Alertmanager components
##
prometheusOperator:
  enabled: true

## Deploy a Prometheus instance
##
prometheus:
  enabled: true
  prometheusSpec:
    ruleSelectorNilUsesHelmValues: false
    serviceMonitorSelectorNilUsesHelmValues: false
    podMonitorSelectorNilUsesHelmValues: false
    probeSelectorNilUsesHelmValues: false
    podMonitorSelector:
      matchLabels: 
        prometheus: "true"
  ## Example service monitors
  ##
  ## Uncomment the following section to enable ingress-nginx service monitoring
  ##
  additionalServiceMonitors:
    - name: "elastic-monitor"
      selector:
        matchLabels:
          app.kubernetes.io/component: metrics
      namespaceSelector:
        matchNames:
          - elastic
      endpoints:
        - port: "http-metrics"
    - name: "nginx-ingress-monitor"
      selector:
        matchLabels:
          app.kubernetes.io/name: ingress-nginx
      namespaceSelector:
        matchNames:
          - ingress-nginx
      endpoints:
        - port: "metrics"


prometheus-node-exporter:
  prometheus:
    monitor:
      relabelings:
      - action: replace
        sourceLabels: [__meta_kubernetes_pod_node_name]
        targetLabel: nodename
  ## Prometheus StorageSpec for persistent data
  ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/user-guides/storage.md
  ##
  prometheusSpec:
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: do-block-storage
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 5Gi

## Configuration for Grafana
## ref: https://grafana.com/
##
## Deploy a Grafana instance
##
grafana:
  enabled: true
  adminPassword: prom-operator # Please change the default password in production !!!
  persistence:
    enabled: true
    storageClassName: do-block-storage
    accessModes: ["ReadWriteOnce"]
    size: 5Gi
    # Provision grafana-dashboards-kubernetes
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
      - name: 'grafana-dashboards-kubernetes'
        orgId: 1
        folder: 'Kubernetes'
        type: file
        disableDeletion: true
        editable: true
        options:
          path: /var/lib/grafana/dashboards/grafana-dashboards-kubernetes
  dashboards:
    grafana-dashboards-kubernetes:
      k8s-system-api-server:
        url: https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-system-api-server.json
        token: ''
      k8s-system-coredns:
        url: https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-system-coredns.json
        token: ''
      k8s-views-global:
        url: https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-views-global.json
        token: ''
      k8s-views-namespaces:
        url: https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-views-namespaces.json
        token: ''
      k8s-views-nodes:
        url: https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-views-nodes.json
        token: ''
      k8s-views-pods:
        url: https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-views-pods.json
        token: ''

## Configuration for Alertmanager
## ref: https://prometheus.io/docs/alerting/alertmanager/
##
## Deploy an Alertmanager instance
##
alertmanager:
  enabled: true
  config:
    global:
      resolve_timeout: 5m
      slack_api_url: "https://hooks.slack.com/services/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"
    route:
      group_by: ['job', 'namespace', 'service', 'pod']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 12h
      receiver: 'slack'
      routes:
        - receiver: 'null'
          matchers:
            - alertname=~"Watchdog|DeadMansSwitch|KubeControllerManagerDown"
    receivers:
      - name: 'null'
      - name: 'slack'
        slack_configs:
        - channel: '#alerts'
          send_resolved: true
          title: '[{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] Monitoring Event Notification'
          text: >-
            {{ range .Alerts }}
              *Alert:* {{ .Annotations.summary }} - `{{ .Labels.severity }}`
              *Description:* {{ .Annotations.description }}
              *Graph:* <{{ .GeneratorURL }}|:chart_with_upwards_trend:> *Runbook:* <{{ .Annotations.runbook }}|:spiral_note_pad:>
              *Details:*
              {{ range .Labels.SortedPairs }} â€¢ *{{ .Name }}:* `{{ .Value }}`
              {{ end }}
            {{ end }}

## Create default rules for monitoring the cluster
##
## Disable `etcd` and `kubeScheduler` rules (managed by DOKS, so metrics are not accessible)
##
defaultRules:
  create: true
  rules:
    etcd: false
    kubeScheduler: false

## Component scraping kube scheduler
##
## Disabled because it's being managed by DOKS, so it's not accessible
##
kubeScheduler:
  enabled: false

## Component scraping etcd
##
## Disabled because it's being managed by DOKS, so it's not accessible
##
kubeEtcd:
  enabled: false


additionalPrometheusRulesMap:
  prometheus.rules:
      groups:
        - name: EmbeddedExporter
          rules:
            - alert: PrometheusTargetMissing
              expr: 'up == 0'
              for: 0m
              labels:
                severity: critical
              annotations:
                summary: Prometheus target missing (instance {{ $labels.instance }})
                description: "A Prometheus target has disappeared. An exporter might be crashed.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PrometheusAllTargetsMissing
              expr: 'sum by (job) (up) == 0'
              for: 0m
              labels:
                severity: critical
              annotations:
                summary: Prometheus all targets missing (instance {{ $labels.instance }})
                description: "A Prometheus job does not have living target anymore.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PrometheusTargetMissingWithWarmupTime
              expr: 'sum by (instance, job) ((up == 0) * on (instance) group_right(job) (node_time_seconds - node_boot_time_seconds > 600))'
              for: 0m
              labels:
                severity: critical
              annotations:
                summary: Prometheus target missing with warmup time (instance {{ $labels.instance }})
                description: "Allow a job time to start up (10 minutes) before alerting that it's down.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PrometheusConfigurationReloadFailure
              expr: 'prometheus_config_last_reload_successful != 1'
              for: 0m
              labels:
                severity: warning
              annotations:
                summary: Prometheus configuration reload failure (instance {{ $labels.instance }})
                description: "Prometheus configuration reload error\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PrometheusTooManyRestarts
              expr: 'changes(process_start_time_seconds{job=~"prometheus|pushgateway|alertmanager"}[15m]) > 2'
              for: 0m
              labels:
                severity: warning
              annotations:
                summary: Prometheus too many restarts (instance {{ $labels.instance }})
                description: "Prometheus has restarted more than twice in the last 15 minutes. It might be crashlooping.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
            - alert: PrometheusAlertmanagerConfigurationReloadFailure
              expr: 'alertmanager_config_last_reload_successful != 1'
              for: 0m
              labels:
                severity: warning
              annotations:
                summary: Prometheus AlertManager configuration reload failure (instance {{ $labels.instance }})
                description: "AlertManager configuration reload error\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PrometheusAlertmanagerConfigNotSynced
              expr: 'count(count_values("config_hash", alertmanager_config_hash)) > 1'
              for: 0m
              labels:
                severity: warning
              annotations:
                summary: Prometheus AlertManager config not synced (instance {{ $labels.instance }})
                description: "Configurations of AlertManager cluster instances are out of sync\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

          
            - alert: PrometheusNotConnectedToAlertmanager
              expr: 'prometheus_notifications_alertmanagers_discovered < 1'
              for: 0m
              labels:
                severity: critical
              annotations:
                summary: Prometheus not connected to alertmanager (instance {{ $labels.instance }})
                description: "Prometheus cannot connect the alertmanager\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PrometheusRuleEvaluationFailures
              expr: 'increase(prometheus_rule_evaluation_failures_total[3m]) > 0'
              for: 0m
              labels:
                severity: critical
              annotations:
                summary: Prometheus rule evaluation failures (instance {{ $labels.instance }})
                description: "Prometheus encountered {{ $value }} rule evaluation failures, leading to potentially ignored alerts.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PrometheusTemplateTextExpansionFailures
              expr: 'increase(prometheus_template_text_expansion_failures_total[3m]) > 0'
              for: 0m
              labels:
                severity: critical
              annotations:
                summary: Prometheus template text expansion failures (instance {{ $labels.instance }})
                description: "Prometheus encountered {{ $value }} template text expansion failures\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PrometheusRuleEvaluationSlow
              expr: 'prometheus_rule_group_last_duration_seconds > prometheus_rule_group_interval_seconds'
              for: 5m
              labels:
                severity: warning
              annotations:
                summary: Prometheus rule evaluation slow (instance {{ $labels.instance }})
                description: "Prometheus rule evaluation took more time than the scheduled interval. It indicates a slower storage backend access or too complex query.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PrometheusNotificationsBacklog
              expr: 'min_over_time(prometheus_notifications_queue_length[10m]) > 0'
              for: 0m
              labels:
                severity: warning
              annotations:
                summary: Prometheus notifications backlog (instance {{ $labels.instance }})
                description: "The Prometheus notification queue has not been empty for 10 minutes\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PrometheusAlertmanagerNotificationFailing
              expr: 'rate(alertmanager_notifications_failed_total[1m]) > 0'
              for: 0m
              labels:
                severity: critical
              annotations:
                summary: Prometheus AlertManager notification failing (instance {{ $labels.instance }})
                description: "Alertmanager is failing sending notifications\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PrometheusTargetEmpty
              expr: 'prometheus_sd_discovered_targets == 0'
              for: 0m
              labels:
                severity: critical
              annotations:
                summary: Prometheus target empty (instance {{ $labels.instance }})
                description: "Prometheus has no target in service discovery\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PrometheusTargetScrapingSlow
              expr: 'prometheus_target_interval_length_seconds{quantile="0.9"} / on (interval, instance, job) prometheus_target_interval_length_seconds{quantile="0.5"} > 1.05'
              for: 5m
              labels:
                severity: warning
              annotations:
                summary: Prometheus target scraping slow (instance {{ $labels.instance }})
                description: "Prometheus is scraping exporters slowly since it exceeded the requested interval time. Your Prometheus server is under-provisioned.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PrometheusLargeScrape
              expr: 'increase(prometheus_target_scrapes_exceeded_sample_limit_total[10m]) > 10'
              for: 5m
              labels:
                severity: warning
              annotations:
                summary: Prometheus large scrape (instance {{ $labels.instance }})
                description: "Prometheus has many scrapes that exceed the sample limit\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PrometheusTargetScrapeDuplicate
              expr: 'increase(prometheus_target_scrapes_sample_duplicate_timestamp_total[5m]) > 0'
              for: 0m
              labels:
                severity: warning
              annotations:
                summary: Prometheus target scrape duplicate (instance {{ $labels.instance }})
                description: "Prometheus has many samples rejected due to duplicate timestamps but different values\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PrometheusTsdbCheckpointCreationFailures
              expr: 'increase(prometheus_tsdb_checkpoint_creations_failed_total[1m]) > 0'
              for: 0m
              labels:
                severity: critical
              annotations:
                summary: Prometheus TSDB checkpoint creation failures (instance {{ $labels.instance }})
                description: "Prometheus encountered {{ $value }} checkpoint creation failures\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PrometheusTsdbCheckpointDeletionFailures
              expr: 'increase(prometheus_tsdb_checkpoint_deletions_failed_total[1m]) > 0'
              for: 0m
              labels:
                severity: critical
              annotations:
                summary: Prometheus TSDB checkpoint deletion failures (instance {{ $labels.instance }})
                description: "Prometheus encountered {{ $value }} checkpoint deletion failures\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PrometheusTsdbCompactionsFailed
              expr: 'increase(prometheus_tsdb_compactions_failed_total[1m]) > 0'
              for: 0m
              labels:
                severity: critical
              annotations:
                summary: Prometheus TSDB compactions failed (instance {{ $labels.instance }})
                description: "Prometheus encountered {{ $value }} TSDB compactions failures\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PrometheusTsdbHeadTruncationsFailed
              expr: 'increase(prometheus_tsdb_head_truncations_failed_total[1m]) > 0'
              for: 0m
              labels:
                severity: critical
              annotations:
                summary: Prometheus TSDB head truncations failed (instance {{ $labels.instance }})
                description: "Prometheus encountered {{ $value }} TSDB head truncation failures\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PrometheusTsdbReloadFailures
              expr: 'increase(prometheus_tsdb_reloads_failures_total[1m]) > 0'
              for: 0m
              labels:
                severity: critical
              annotations:
                summary: Prometheus TSDB reload failures (instance {{ $labels.instance }})
                description: "Prometheus encountered {{ $value }} TSDB reload failures\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PrometheusTsdbWalCorruptions
              expr: 'increase(prometheus_tsdb_wal_corruptions_total[1m]) > 0'
              for: 0m
              labels:
                severity: critical
              annotations:
                summary: Prometheus TSDB WAL corruptions (instance {{ $labels.instance }})
                description: "Prometheus encountered {{ $value }} TSDB WAL corruptions\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PrometheusTsdbWalTruncationsFailed
              expr: 'increase(prometheus_tsdb_wal_truncations_failed_total[1m]) > 0'
              for: 0m
              labels:
                severity: critical
              annotations:
                summary: Prometheus TSDB WAL truncations failed (instance {{ $labels.instance }})
                description: "Prometheus encountered {{ $value }} TSDB WAL truncation failures\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: PrometheusTimeseriesCardinality
              expr: 'label_replace(count by(__name__) ({__name__=~".+"}), "name", "$1", "__name__", "(.+)") > 10000'
              for: 0m
              labels:
                severity: warning
              annotations:
                summary: Prometheus timeseries cardinality (instance {{ $labels.instance }})
                description: "The \"{{ $labels.name }}\" timeseries cardinality is getting very high: {{ $value }}\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
        - name: KubestateExporter
          rules:
            - alert: KubernetesNodeNotReady
              expr: 'kube_node_status_condition{condition="Ready",status="true"} == 0'
              for: 10m
              labels:
                severity: critical
              annotations:
                summary: Kubernetes Node ready (node {{ $labels.node }})
                description: "Node {{ $labels.node }} has been unready for a long time\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: KubernetesNodeMemoryPressure
              expr: 'kube_node_status_condition{condition="MemoryPressure",status="true"} == 1'
              for: 2m
              labels:
                severity: critical
              annotations:
                summary: Kubernetes memory pressure (node {{ $labels.node }})
                description: "Node {{ $labels.node }} has MemoryPressure condition\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: KubernetesNodeDiskPressure
              expr: 'kube_node_status_condition{condition="DiskPressure",status="true"} == 1'
              for: 2m
              labels:
                severity: critical
              annotations:
                summary: Kubernetes disk pressure (node {{ $labels.node }})
                description: "Node {{ $labels.node }} has DiskPressure condition\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: KubernetesNodeNetworkUnavailable
              expr: 'kube_node_status_condition{condition="NetworkUnavailable",status="true"} == 1'
              for: 2m
              labels:
                severity: critical
              annotations:
                summary: Kubernetes Node network unavailable (instance {{ $labels.instance }})
                description: "Node {{ $labels.node }} has NetworkUnavailable condition\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: KubernetesNodeOutOfPodCapacity
              expr: 'sum by (node) ((kube_pod_status_phase{phase="Running"} == 1) + on(uid) group_left(node) (0 * kube_pod_info{pod_template_hash=""})) / sum by (node) (kube_node_status_allocatable{resource="pods"}) * 100 > 90'
              for: 2m
              labels:
                severity: warning
              annotations:
                summary: Kubernetes Node out of pod capacity (instance {{ $labels.instance }})
                description: "Node {{ $labels.node }} is out of pod capacity\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: KubernetesContainerOomKiller
              expr: '(kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 10m >= 1) and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason="OOMKilled"}[10m]) == 1'
              for: 0m
              labels:
                severity: warning
              annotations:
                summary: Kubernetes container oom killer ({{ $labels.namespace }}/{{ $labels.pod }}:{{ $labels.container }})
                description: "Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} has been OOMKilled {{ $value }} times in the last 10 minutes.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: KubernetesJobFailed
              expr: 'kube_job_status_failed > 0'
              for: 0m
              labels:
                severity: warning
              annotations:
                summary: Kubernetes Job failed ({{ $labels.namespace }}/{{ $labels.job_name }})
                description: "Job {{ $labels.namespace }}/{{ $labels.job_name }} failed to complete\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: KubernetesCronjobSuspended
              expr: 'kube_cronjob_spec_suspend != 0'
              for: 0m
              labels:
                severity: warning
              annotations:
                summary: Kubernetes CronJob suspended ({{ $labels.namespace }}/{{ $labels.cronjob }})
                description: "CronJob {{ $labels.namespace }}/{{ $labels.cronjob }} is suspended\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: KubernetesPersistentvolumeclaimPending
              expr: 'kube_persistentvolumeclaim_status_phase{phase="Pending"} == 1'
              for: 2m
              labels:
                severity: warning
              annotations:
                summary: Kubernetes PersistentVolumeClaim pending ({{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }})
                description: "PersistentVolumeClaim {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is pending\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: KubernetesVolumeOutOfDiskSpace
              expr: 'kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes * 100 < 10'
              for: 2m
              labels:
                severity: warning
              annotations:
                summary: Kubernetes Volume out of disk space (instance {{ $labels.instance }})
                description: "Volume is almost full (< 10% left)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: KubernetesVolumeFullInFourDays
              expr: 'predict_linear(kubelet_volume_stats_available_bytes[6h:5m], 4 * 24 * 3600) < 0'
              for: 0m
              labels:
                severity: critical
              annotations:
                summary: Kubernetes Volume full in four days (instance {{ $labels.instance }})
                description: "Volume under {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is expected to fill up within four days. Currently {{ $value | humanize }}% is available.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: KubernetesPersistentvolumeError
              expr: 'kube_persistentvolume_status_phase{phase=~"Failed|Pending", job="kube-state-metrics"} > 0'
              for: 0m
              labels:
                severity: critical
              annotations:
                summary: Kubernetes PersistentVolumeClaim pending ({{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }})
                description: "Persistent volume {{ $labels.persistentvolume }} is in bad state\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: KubernetesStatefulsetDown
              expr: 'kube_statefulset_replicas != kube_statefulset_status_replicas_ready > 0'
              for: 1m
              labels:
                severity: critical
              annotations:
                summary: Kubernetes StatefulSet down ({{ $labels.namespace }}/{{ $labels.statefulset }})
                description: "StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} went down\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: KubernetesHpaScaleInability
              expr: 'kube_horizontalpodautoscaler_status_condition{status="false", condition="AbleToScale"} == 1'
              for: 2m
              labels:
                severity: warning
              annotations:
                summary: Kubernetes HPA scale inability (instance {{ $labels.instance }})
                description: "HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler }} is unable to scale\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: KubernetesHpaMetricsUnavailability
              expr: 'kube_horizontalpodautoscaler_status_condition{status="false", condition="ScalingActive"} == 1'
              for: 0m
              labels:
                severity: warning
              annotations:
                summary: Kubernetes HPA metrics unavailability (instance {{ $labels.instance }})
                description: "HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler }} is unable to collect metrics\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: KubernetesHpaScaleMaximum
              expr: 'kube_horizontalpodautoscaler_status_desired_replicas >= kube_horizontalpodautoscaler_spec_max_replicas'
              for: 2m
              labels:
                severity: info
              annotations:
                summary: Kubernetes HPA scale maximum (instance {{ $labels.instance }})
                description: "HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler }} has hit maximum number of desired pods\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: KubernetesHpaUnderutilized
              expr: 'max(quantile_over_time(0.5, kube_horizontalpodautoscaler_status_desired_replicas[1d]) == kube_horizontalpodautoscaler_spec_min_replicas) by (horizontalpodautoscaler) > 3'
              for: 0m
              labels:
                severity: info
              annotations:
                summary: Kubernetes HPA underutilized (instance {{ $labels.instance }})
                description: "HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler }} is constantly at minimum replicas for 50% of the time. Potential cost saving here.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: KubernetesPodNotHealthy
              expr: 'sum by (namespace, pod) (kube_pod_status_phase{phase=~"Pending|Unknown|Failed"}) > 0'
              for: 15m
              labels:
                severity: critical
              annotations:
                summary: Kubernetes Pod not healthy ({{ $labels.namespace }}/{{ $labels.pod }})
                description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-running state for longer than 15 minutes.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: KubernetesPodCrashLooping
              expr: 'increase(kube_pod_container_status_restarts_total[1m]) > 3'
              for: 2m
              labels:
                severity: warning
              annotations:
                summary: Kubernetes pod crash looping ({{ $labels.namespace }}/{{ $labels.pod }})
                description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is crash looping\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: KubernetesReplicasetReplicasMismatch
              expr: 'kube_replicaset_spec_replicas != kube_replicaset_status_ready_replicas'
              for: 10m
              labels:
                severity: warning
              annotations:
                summary: Kubernetes ReplicasSet mismatch ({{ $labels.namespace }}/{{ $labels.replicaset }})
                description: "ReplicaSet {{ $labels.namespace }}/{{ $labels.replicaset }} replicas mismatch\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: KubernetesDeploymentReplicasMismatch
              expr: 'kube_deployment_spec_replicas != kube_deployment_status_replicas_available'
              for: 10m
              labels:
                severity: warning
              annotations:
                summary: Kubernetes Deployment replicas mismatch ({{ $labels.namespace }}/{{ $labels.deployment }})
                description: "Deployment {{ $labels.namespace }}/{{ $labels.deployment }} replicas mismatch\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: KubernetesStatefulsetReplicasMismatch
              expr: 'kube_statefulset_status_replicas_ready != kube_statefulset_status_replicas'
              for: 10m
              labels:
                severity: warning
              annotations:
                summary: Kubernetes StatefulSet replicas mismatch (instance {{ $labels.instance }})
                description: "StatefulSet does not match the expected number of replicas.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: KubernetesDeploymentGenerationMismatch
              expr: 'kube_deployment_status_observed_generation != kube_deployment_metadata_generation'
              for: 10m
              labels:
                severity: critical
              annotations:
                summary: Kubernetes Deployment generation mismatch ({{ $labels.namespace }}/{{ $labels.deployment }})
                description: "Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has failed but has not been rolled back.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: KubernetesStatefulsetGenerationMismatch
              expr: 'kube_statefulset_status_observed_generation != kube_statefulset_metadata_generation'
              for: 10m
              labels:
                severity: critical
              annotations:
                summary: Kubernetes StatefulSet generation mismatch ({{ $labels.namespace }}/{{ $labels.statefulset }})
                description: "StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} has failed but has not been rolled back.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: KubernetesStatefulsetUpdateNotRolledOut
              expr: 'max without (revision) (kube_statefulset_status_current_revision unless kube_statefulset_status_update_revision) * (kube_statefulset_replicas != kube_statefulset_status_replicas_updated)'
              for: 10m
              labels:
                severity: warning
              annotations:
                summary: Kubernetes StatefulSet update not rolled out ({{ $labels.namespace }}/{{ $labels.statefulset }})
                description: "StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} update has not been rolled out.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: KubernetesDaemonsetRolloutStuck
              expr: 'kube_daemonset_status_number_ready / kube_daemonset_status_desired_number_scheduled * 100 < 100 or kube_daemonset_status_desired_number_scheduled - kube_daemonset_status_current_number_scheduled > 0'
              for: 10m
              labels:
                severity: warning
              annotations:
                summary: Kubernetes DaemonSet rollout stuck ({{ $labels.namespace }}/{{ $labels.daemonset }})
                description: "Some Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are not scheduled or not ready\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: KubernetesDaemonsetMisscheduled
              expr: 'kube_daemonset_status_number_misscheduled > 0'
              for: 1m
              labels:
                severity: critical
              annotations:
                summary: Kubernetes DaemonSet misscheduled ({{ $labels.namespace }}/{{ $labels.daemonset }})
                description: "Some Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are running where they are not supposed to run\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: KubernetesCronjobTooLong
              expr: 'time() - kube_cronjob_next_schedule_time > 3600'
              for: 0m
              labels:
                severity: warning
              annotations:
                summary: Kubernetes CronJob too long ({{ $labels.namespace }}/{{ $labels.cronjob }})
                description: "CronJob {{ $labels.namespace }}/{{ $labels.cronjob }} is taking more than 1h to complete.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: KubernetesJobSlowCompletion
              expr: 'kube_job_spec_completions - kube_job_status_succeeded - kube_job_status_failed > 0'
              for: 12h
              labels:
                severity: critical
              annotations:
                summary: Kubernetes job slow completion ({{ $labels.namespace }}/{{ $labels.job_name }})
                description: "Kubernetes Job {{ $labels.namespace }}/{{ $labels.job_name }} did not complete in time.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: KubernetesApiServerErrors
              expr: 'sum(rate(apiserver_request_total{job="apiserver",code=~"^(?:5..)$"}[1m])) / sum(rate(apiserver_request_total{job="apiserver"}[1m])) * 100 > 3'
              for: 2m
              labels:
                severity: critical
              annotations:
                summary: Kubernetes API server errors (instance {{ $labels.instance }})
                description: "Kubernetes API server is experiencing high error rate\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: KubernetesApiClientErrors
              expr: '(sum(rate(rest_client_requests_total{code=~"(4|5).."}[1m])) by (instance, job) / sum(rate(rest_client_requests_total[1m])) by (instance, job)) * 100 > 1'
              for: 2m
              labels:
                severity: critical
              annotations:
                summary: Kubernetes API client errors (instance {{ $labels.instance }})
                description: "Kubernetes API client is experiencing high error rate\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: KubernetesClientCertificateExpiresNextWeek
              expr: 'apiserver_client_certificate_expiration_seconds_count{job="apiserver"} > 0 and histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m]))) < 7*24*60*60'
              for: 0m
              labels:
                severity: warning
              annotations:
                summary: Kubernetes client certificate expires next week (instance {{ $labels.instance }})
                description: "A client certificate used to authenticate to the apiserver is expiring next week.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: KubernetesClientCertificateExpiresSoon
              expr: 'apiserver_client_certificate_expiration_seconds_count{job="apiserver"} > 0 and histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m]))) < 24*60*60'
              for: 0m
              labels:
                severity: critical
              annotations:
                summary: Kubernetes client certificate expires soon (instance {{ $labels.instance }})
                description: "A client certificate used to authenticate to the apiserver is expiring in less than 24.0 hours.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

            - alert: KubernetesApiServerLatency
              expr: 'histogram_quantile(0.99, sum(rate(apiserver_request_duration_seconds_bucket{subresource!="log",verb!~"^(?:CONNECT|WATCHLIST|WATCH|PROXY)$"} [5m])) WITHOUT (instance, resource)) > 1'
              for: 2m
              labels:
                severity: warning
              annotations:
                summary: Kubernetes API server latency (instance {{ $labels.instance }})
                description: "Kubernetes API server has a 99th percentile latency of {{ $value }} seconds for {{ $labels.verb }} {{ $labels.resource }}.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"


                